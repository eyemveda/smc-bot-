{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIsTHBRsDgs38BoacX7gXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyemveda/smc-bot-/blob/main/Backtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install backtesting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1REAf1AbYzV",
        "outputId": "6655cf69-6038-48f4-ec71-667d2683a975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting backtesting\n",
            "  Downloading backtesting-0.6.5-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from backtesting) (2.0.2)\n",
            "Requirement already satisfied: pandas!=0.25.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from backtesting) (2.2.2)\n",
            "Requirement already satisfied: bokeh!=3.0.*,!=3.2.*,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from backtesting) (3.7.3)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=3.0.0->backtesting) (3.1.6)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=3.0.0->backtesting) (1.3.3)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=3.0.0->backtesting) (2.0.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=3.0.0->backtesting) (25.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=3.0.0->backtesting) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=3.0.0->backtesting) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=3.0.0->backtesting) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=3.0.0->backtesting) (2025.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=0.25.0,>=0.25.0->backtesting) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=0.25.0,>=0.25.0->backtesting) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=0.25.0,>=0.25.0->backtesting) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh!=3.0.*,!=3.2.*,>=3.0.0->backtesting) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=0.25.0,>=0.25.0->backtesting) (1.17.0)\n",
            "Downloading backtesting-0.6.5-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.1/192.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: backtesting\n",
            "Successfully installed backtesting-0.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import re\n",
        "from backtesting import Backtest, Strategy"
      ],
      "metadata": {
        "id": "qRCTqp9RUqTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "064841c0-4680-41ff-c242-36899dfffcd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/backtesting/_plotting.py:55: UserWarning: Jupyter Notebook detected. Setting Bokeh output to notebook. This may not work in Jupyter clients without JavaScript support, such as old IDEs. Reset with `backtesting.set_bokeh_output(notebook=False)`.\n",
            "  warnings.warn('Jupyter Notebook detected. '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file with tab separator, skipping the first row\n",
        "df_raw = pd.read_csv(\"/content/XAUUSDm_M1_2025.csv\", sep='\\t', engine='python', header=None, skiprows=1)\n",
        "\n",
        "# Drop empty columns caused by separator - this might not be needed with correct sep\n",
        "# df_raw = df_raw.dropna(axis=1, how='all')\n",
        "\n",
        "# Rename columns manually - assuming the tab separation results in 9 columns\n",
        "df_raw.columns = [\"DATE\", \"TIME\", \"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\", \"TICKVOL\", \"VOL\", \"SPREAD\"]\n",
        "\n",
        "# Combine DATE and TIME into a single datetime column\n",
        "df_raw[\"DateTime\"] = pd.to_datetime(df_raw[\"DATE\"] + \" \" + df_raw[\"TIME\"])\n",
        "\n",
        "# Set index and clean up\n",
        "df_1m = df_raw.set_index(\"DateTime\")\n",
        "df_1m = df_1m.rename(columns=str.lower)\n",
        "# Select relevant columns, making sure they are in lowercase after renaming\n",
        "df_1m = df_1m[['open', 'high', 'low', 'close', 'vol']]"
      ],
      "metadata": {
        "id": "ava9oE72XGN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizing for multitimeframe analysis\n",
        "def resample_to_tf(df, timeframe):\n",
        "    return df.resample(timeframe).agg({\n",
        "        'open': 'first',\n",
        "        'high': 'max',\n",
        "        'low': 'min',\n",
        "        'close': 'last',\n",
        "        'vol': 'sum' # Changed 'volume' to 'vol'\n",
        "    }).dropna()\n",
        "\n",
        "df_5m = resample_to_tf(df_1m, \"5min\") # Changed \"5T\" to \"5min\"\n",
        "df_15m = resample_to_tf(df_1m, \"15min\") # Changed \"15T\" to \"15min\"\n",
        "df_1h = resample_to_tf(df_1m, \"1H\")\n",
        "df_4h = resample_to_tf(df_1m, \"4H\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FueFgREsYqzU",
        "outputId": "44a841d9-eb39-4971-ec99-aed71f7e1d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2842005807.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  return df.resample(timeframe).agg({\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing the Warning In the Above Cell\n",
        "# Fixing the Warning\n",
        "def normalize_tf(tf: str) -> str:\n",
        "    import re\n",
        "\n",
        "    if not isinstance(tf, str):\n",
        "        return tf\n",
        "    s = tf.strip().lower()\n",
        "    m = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', s)\n",
        "    if not m:\n",
        "        return s  # leave as-is, let pandas raise if invalid\n",
        "\n",
        "    n, unit = m.groups()\n",
        "\n",
        "    # Map to pandas-friendly units (lowercase for everything except 'M' for months)\n",
        "    unit_map = {\n",
        "        't': 'min', 't.': 'min', 'min': 'min', 'm': 'min', 'minute': 'min', 'minutes': 'min',\n",
        "        'h': 'h', 'hour': 'h', 'hours': 'h',\n",
        "        'd': 'd', 'day': 'd', 'days': 'd',\n",
        "        'w': 'w', 'week': 'w', 'weeks': 'w',\n",
        "        'mo': 'M', 'month': 'M', 'months': 'M'\n",
        "    }\n",
        "\n",
        "    unit_norm = unit_map.get(unit, unit)\n",
        "    return f\"{n}{unit_norm}\"\n"
      ],
      "metadata": {
        "id": "VXTEpQ69ZhNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# atr stuff\n",
        "\n",
        "def atr(df, period=14):\n",
        "    \"\"\"\n",
        "    Average True Range (ATR) for backtesting & live trading.\n",
        "    Uses Welles Wilder's method with proper shift to avoid lookahead.\n",
        "    \"\"\"\n",
        "    if df is None or len(df) < 2:\n",
        "        return pd.Series(dtype=float)\n",
        "\n",
        "    high = df['high']\n",
        "    low = df['low']\n",
        "    close = df['close']\n",
        "\n",
        "    tr = pd.concat([\n",
        "        (high - low).abs(),\n",
        "        (high - close.shift(1)).abs(),\n",
        "        (low - close.shift(1)).abs()\n",
        "    ], axis=1).max(axis=1)\n",
        "\n",
        "    # Wilder's smoothing (EMA-like) for classic ATR\n",
        "    atr_values = tr.ewm(alpha=1/period, adjust=False).mean()\n",
        "\n",
        "    # Shift so the ATR at bar X only uses info up to bar X-1\n",
        "    atr_values = atr_values.shift(1)\n",
        "\n",
        "    return atr_values"
      ],
      "metadata": {
        "id": "pgX0NOCGivgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Liquidity Sweep Detection Logic"
      ],
      "metadata": {
        "id": "Ru8U0oTYbkNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_ohlcv(df_1m, timeframe):\n",
        "    tf = normalize_tf(timeframe)\n",
        "    agg = {'open':'first','high':'max','low':'min','close':'last'}\n",
        "    if 'volume' in df_1m.columns: agg['volume']='sum'\n",
        "    res = df_1m.resample(tf).agg(agg)\n",
        "    res = res.dropna(subset=['open','high','low','close'])\n",
        "    return res\n",
        "\n",
        "def atr_series(df, n=14):\n",
        "    h,l,c = df['high'], df['low'], df['close']\n",
        "    tr = pd.concat([h-l, (h-c.shift()).abs(), (l-c.shift()).abs()], axis=1).max(axis=1)\n",
        "    return tr.ewm(span=n, adjust=False).mean()\n",
        "\n",
        "def zigzag_pivots_from_df(df_tf, pct=0.004, use_atr=False, atr_period=14):\n",
        "    if df_tf.empty:\n",
        "        return []\n",
        "    closes = df_tf['close'].values\n",
        "    idx = list(df_tf.index)\n",
        "    pivots = []\n",
        "    last_price = closes[0]\n",
        "    last_idx = 0\n",
        "    direction = None\n",
        "    atr = atr_series(df_tf, n=atr_period).reindex(df_tf.index).bfill().values if use_atr else None\n",
        "\n",
        "    for i in range(1, len(closes)):\n",
        "        threshold = (atr[i] if use_atr else closes[i] * pct)\n",
        "        if threshold <= 0:\n",
        "            continue\n",
        "        change = closes[i] - last_price\n",
        "        if direction is None:\n",
        "            if abs(change) >= threshold:\n",
        "                direction = 'up' if change > 0 else 'down'\n",
        "                pivots.append((idx[last_idx], float(last_price), 'low' if direction=='up' else 'high'))\n",
        "                last_price = closes[i]; last_idx = i\n",
        "        elif direction == 'up':\n",
        "            if closes[i] > last_price:\n",
        "                last_price = closes[i]; last_idx = i\n",
        "            elif (last_price - closes[i]) >= threshold:\n",
        "                pivots.append((idx[last_idx], float(last_price), 'high'))\n",
        "                direction = 'down'\n",
        "                last_price = closes[i]; last_idx = i\n",
        "        else:  # down\n",
        "            if closes[i] < last_price:\n",
        "                last_price = closes[i]; last_idx = i\n",
        "            elif (closes[i] - last_price) >= threshold:\n",
        "                pivots.append((idx[last_idx], float(last_price), 'low'))\n",
        "                direction = 'up'\n",
        "                last_price = closes[i]; last_idx = i\n",
        "    return pivots\n",
        "\n",
        "def is_level_swept(level_price, level_ts, direction, df_1m, tol_abs=0.0):\n",
        "    mask = df_1m.index > pd.Timestamp(level_ts)\n",
        "    if mask.sum() == 0:\n",
        "        return False\n",
        "    sub = df_1m.loc[mask]\n",
        "    if direction == 'high':\n",
        "        return (sub['high'] >= (level_price + tol_abs)).any()\n",
        "    else:\n",
        "        return (sub['low'] <= (level_price - tol_abs)).any()\n",
        "\n",
        "def cluster_levels(levels, tol):\n",
        "    if not levels:\n",
        "        return []\n",
        "    levels = sorted(levels)\n",
        "    clusters=[]\n",
        "    for p in levels:\n",
        "        if not clusters:\n",
        "            clusters.append([p])\n",
        "        else:\n",
        "            if abs(p - np.mean(clusters[-1])) <= tol:\n",
        "                clusters[-1].append(p)\n",
        "            else:\n",
        "                clusters.append([p])\n",
        "    return [{'level': float(np.mean(c)), 'count': len(c)} for c in clusters]\n",
        "\n",
        "def build_mtf_liquidity_debug(df_1m,\n",
        "                        use_tfs = ['4h','1h'],\n",
        "                        include_15m_for_tp=True,\n",
        "                        zigzag_pct_val = 0.003,   # start slightly lower\n",
        "                        zigzag_use_atr = True,\n",
        "                        atr_period = 14,\n",
        "                        equal_tol_atr_mult=0.30,\n",
        "                        verbose=True):\n",
        "    atr_1m = atr_series(df_1m, n=atr_period)\n",
        "    median_atr = float(atr_1m.median())\n",
        "    tol = max(median_atr * equal_tol_atr_mult, 1e-6)\n",
        "\n",
        "    all_pivots = []\n",
        "    debug = {'resampled_rows':{}, 'raw_pivots_per_tf':{}}\n",
        "\n",
        "    for tf in use_tfs:\n",
        "        df_tf = resample_ohlcv(df_1m, tf)\n",
        "        debug['resampled_rows'][tf] = len(df_tf)\n",
        "        piv = zigzag_pivots_from_df(df_tf, pct=zigzag_pct_val, use_atr=zigzag_use_atr, atr_period=atr_period)\n",
        "        debug['raw_pivots_per_tf'][tf] = len(piv)\n",
        "        all_pivots += [(ts, price, typ, tf) for (ts, price, typ) in piv]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"median ATR (1m):\", median_atr)\n",
        "        print(\"clustering tol:\", tol)\n",
        "        for tf in use_tfs:\n",
        "            print(f\"Resampled {tf}: {debug['resampled_rows'].get(tf,0)} rows, pivots found: {debug['raw_pivots_per_tf'].get(tf,0)}\")\n",
        "\n",
        "    pivot_rows = []\n",
        "    for ts, price, typ, tf in all_pivots:\n",
        "        swept = is_level_swept(price, ts, typ, df_1m, tol_abs=tol*0.5)\n",
        "        pivot_rows.append({'ts':pd.Timestamp(ts),'price':float(price),'type':typ,'tf':tf,'swept':swept})\n",
        "    pivots_df = pd.DataFrame(pivot_rows).sort_values(['tf','ts'], ascending=[True,True]).reset_index(drop=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"total pivots (all TFs):\", len(pivots_df))\n",
        "        if not pivots_df.empty:\n",
        "            print(\"swept count:\", pivots_df['swept'].sum(), \"unswept:\", (~pivots_df['swept']).sum())\n",
        "\n",
        "    pivots_unswept = pivots_df[~pivots_df['swept']].copy()\n",
        "    high_prices = list(pivots_unswept[pivots_unswept['type']=='high']['price'])\n",
        "    low_prices  = list(pivots_unswept[pivots_unswept['type']=='low']['price'])\n",
        "    high_clusters = cluster_levels(high_prices, tol)\n",
        "    low_clusters  = cluster_levels(low_prices, tol)\n",
        "\n",
        "    rows=[]\n",
        "    for c in high_clusters:\n",
        "        rows.append({'level':c['level'],'side':'sell','count':c['count'],'source':'equal_highs','strength':1 + c['count']/2})\n",
        "    for c in low_clusters:\n",
        "        rows.append({'level':c['level'],'side':'buy','count':c['count'],'source':'equal_lows','strength':1 + c['count']/2})\n",
        "\n",
        "    liquidity_df = pd.DataFrame(rows).sort_values('strength', ascending=False).reset_index(drop=True) if rows else pd.DataFrame(columns=['level','side','count','source','strength'])\n",
        "\n",
        "    # 15min TP pivots\n",
        "    tp_15m = pd.DataFrame()\n",
        "    if include_15m_for_tp:\n",
        "        df_15m = resample_ohlcv(df_1m, '15min')\n",
        "        piv15 = zigzag_pivots_from_df(df_15m, pct=zigzag_pct_val/2, use_atr=zigzag_use_atr, atr_period=atr_period)\n",
        "        rows15=[]\n",
        "        for ts,price,typ in piv15:\n",
        "            swept = is_level_swept(price, ts, typ, df_1m, tol_abs=tol*0.5)\n",
        "            if not swept:\n",
        "                rows15.append({'ts':pd.Timestamp(ts),'price':price,'type':typ})\n",
        "        if rows15:\n",
        "            tp_15m = pd.DataFrame(rows15)\n",
        "    return liquidity_df, tp_15m, pivots_df, pivots_unswept, debug"
      ],
      "metadata": {
        "id": "wGhB8rtIbrEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fairvalue Gap Detection\n"
      ],
      "metadata": {
        "id": "tONqRmDNh4ix"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a5369bf"
      },
      "source": [
        "  # ---------------- Fair Value Gap (FVG) Detection ----------------\n",
        "def identify_fvg(df, min_gap_pct=0.0, use_atr_filter=False, atr_period=14, atr_mult=0.5, tol_abs=None):\n",
        "    \"\"\"\n",
        "    Identify bullish/bearish Fair Value Gaps (FVGs) and optionally filter by size/ATR or sweep status.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with columns:\n",
        "        - fvg_type: 1 = bullish, -1 = bearish, 0 = none\n",
        "        - fvg_top, fvg_bottom: FVG boundaries\n",
        "        - fvg_mid: midpoint of the gap\n",
        "        - fvg_range: size of the gap\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    if len(df) < 3:\n",
        "        df[['fvg_type', 'fvg_top', 'fvg_bottom', 'fvg_mid', 'fvg_range']] = np.nan\n",
        "        df['fvg_type'] = 0\n",
        "        return df\n",
        "\n",
        "    # Optional ATR for filtering\n",
        "    atr_vals = atr(df, period=atr_period) if use_atr_filter else None\n",
        "\n",
        "    bullish = (df['low'] > df['high'].shift(2))\n",
        "    bearish = (df['high'] < df['low'].shift(2))\n",
        "\n",
        "    df['fvg_type'] = 0\n",
        "    df[['fvg_top', 'fvg_bottom', 'fvg_mid', 'fvg_range']] = np.nan\n",
        "\n",
        "    # --- Bullish FVG ---\n",
        "    for idx in df[bullish].index:\n",
        "        gap_size = df.at[idx, 'low'] - df.at[idx - 2, 'high']\n",
        "        if min_gap_pct > 0 and (gap_size / df.at[idx, 'low']) < min_gap_pct:\n",
        "            continue\n",
        "        if use_atr_filter and gap_size < atr_vals.at[idx] * atr_mult:\n",
        "            continue\n",
        "        # Sweep check: has price closed through the gap?\n",
        "        if tol_abs is not None:\n",
        "            swept = (df.loc[idx:, 'low'] <= df.at[idx - 2, 'high'] + tol_abs).any()\n",
        "            if swept:\n",
        "                continue\n",
        "        df.at[idx, 'fvg_type'] = 1\n",
        "        df.at[idx, 'fvg_bottom'] = df.at[idx - 2, 'high']\n",
        "        df.at[idx, 'fvg_top'] = df.at[idx, 'low']\n",
        "        df.at[idx, 'fvg_mid'] = (df.at[idx, 'low'] + df.at[idx - 2, 'high']) / 2\n",
        "        df.at[idx, 'fvg_range'] = gap_size\n",
        "\n",
        "    # --- Bearish FVG ---\n",
        "    for idx in df[bearish].index:\n",
        "        gap_size = df.at[idx - 2, 'low'] - df.at[idx, 'high']\n",
        "        if min_gap_pct > 0 and (gap_size / df.at[idx, 'high']) < min_gap_pct:\n",
        "            continue\n",
        "        if use_atr_filter and gap_size < atr_vals.at[idx] * atr_mult:\n",
        "            continue\n",
        "        if tol_abs is not None:\n",
        "            swept = (df.loc[idx:, 'high'] >= df.at[idx - 2, 'low'] - tol_abs).any()\n",
        "            if swept:\n",
        "                continue\n",
        "        df.at[idx, 'fvg_type'] = -1\n",
        "        df.at[idx, 'fvg_bottom'] = df.at[idx - 2, 'low']\n",
        "        df.at[idx, 'fvg_top'] = df.at[idx, 'high']\n",
        "        df.at[idx, 'fvg_mid'] = (df.at[idx, 'high'] + df.at[idx - 2, 'low']) / 2\n",
        "        df.at[idx, 'fvg_range'] = gap_size\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retracements In FVG"
      ],
      "metadata": {
        "id": "eMJ6Nly41Y6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def track_fvg_fills(df, fvg_col_type='fvg_type', top_col='fvg_top', bottom_col='fvg_bottom'):\n",
        "    \"\"\"\n",
        "    Tracks whether Fair Value Gaps (FVGs) have been retraced/filled.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Must have columns [fvg_type, fvg_top, fvg_bottom]\n",
        "        fvg_col_type: column name for FVG type (1 = bullish, -1 = bearish)\n",
        "        top_col: top boundary of gap\n",
        "        bottom_col: bottom boundary of gap\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: same DF with new 'fvg_filled' (bool) column\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['fvg_filled'] = False  # default\n",
        "\n",
        "    active_fvgs = []  # store (index, type, top, bottom)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        #  Add new FVG if found\n",
        "        if row[fvg_col_type] != 0 and not np.isnan(row[top_col]) and not np.isnan(row[bottom_col]):\n",
        "            active_fvgs.append({\n",
        "                'created_at': idx,\n",
        "                'type': row[fvg_col_type],\n",
        "                'top': row[top_col],\n",
        "                'bottom': row[bottom_col],\n",
        "                'filled': False\n",
        "            })\n",
        "\n",
        "        #  Check active FVGs for retrace/fill\n",
        "        for fvg in active_fvgs:\n",
        "            if fvg['filled']:\n",
        "                continue  # already filled, skip\n",
        "\n",
        "            if fvg['type'] == 1:  # bullish FVG\n",
        "                if row['low'] <= fvg['top']:\n",
        "                    fvg['filled'] = True\n",
        "\n",
        "            elif fvg['type'] == -1:  # bearish FVG\n",
        "                if row['high'] >= fvg['bottom']:\n",
        "                    fvg['filled'] = True\n",
        "\n",
        "        # Mark fills in df\n",
        "        for fvg in active_fvgs:\n",
        "            if fvg['filled'] and idx >= fvg['created_at']:\n",
        "                df.at[idx, 'fvg_filled'] = True\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "CdJzM_wQ1cmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inverse Fair Value Gaps (IFVGs)"
      ],
      "metadata": {
        "id": "Vhlb3bBA0T6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_ifvg(df, reference_break_time=None, min_gap_pct=0.0, use_atr_filter=False, atr_period=14, atr_mult=0.5):\n",
        "    \"\"\"\n",
        "    Identify inverse Fair Value Gaps (iFVGs) — FVGs created before a break of structure,\n",
        "    which remain unmitigated until after the break.\n",
        "\n",
        "    Parameters:\n",
        "        df : DataFrame with OHLC\n",
        "        reference_break_time : optional pd.Timestamp, FVG must occur before this time\n",
        "        min_gap_pct : minimum gap size as fraction of price to be considered\n",
        "        use_atr_filter : whether to filter by ATR size\n",
        "        atr_period : ATR period if ATR filter is used\n",
        "        atr_mult : ATR multiplier for filtering\n",
        "    \"\"\"\n",
        "    df = identify_fvg(df.copy(), min_gap_pct=min_gap_pct, use_atr_filter=use_atr_filter,\n",
        "                      atr_period=atr_period, atr_mult=atr_mult)\n",
        "\n",
        "    if 'fvg_type' not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Filter to actual FVGs\n",
        "    fvg_candidates = df[df['fvg_type'] != 0].copy()\n",
        "    if fvg_candidates.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    fvg_candidates['mitigated'] = False\n",
        "    fvg_candidates['mitigated_at'] = pd.NaT\n",
        "\n",
        "    index_list = list(df.index)  # avoid .get_loc in loop\n",
        "\n",
        "    for i, idx in enumerate(index_list):\n",
        "        if idx not in fvg_candidates.index:\n",
        "            continue\n",
        "\n",
        "        ob_top = fvg_candidates.at[idx, 'fvg_top']\n",
        "        ob_bottom = fvg_candidates.at[idx, 'fvg_bottom']\n",
        "        fvg_type = fvg_candidates.at[idx, 'fvg_type']\n",
        "\n",
        "        # Check for mitigation in future bars\n",
        "        for j in range(i+1, len(index_list)):\n",
        "            c = df.iloc[j]\n",
        "            if pd.isna(ob_top) or pd.isna(ob_bottom):\n",
        "                break\n",
        "\n",
        "            if fvg_type == 1:  # bullish\n",
        "                if c['low'] <= ob_top:\n",
        "                    fvg_candidates.at[idx, 'mitigated'] = True\n",
        "                    fvg_candidates.at[idx, 'mitigated_at'] = index_list[j]\n",
        "                    break\n",
        "            elif fvg_type == -1:  # bearish\n",
        "                if c['high'] >= ob_bottom:\n",
        "                    fvg_candidates.at[idx, 'mitigated'] = True\n",
        "                    fvg_candidates.at[idx, 'mitigated_at'] = index_list[j]\n",
        "                    break\n",
        "\n",
        "    # Keep unmitigated before break time (if provided)\n",
        "    if reference_break_time is not None:\n",
        "        if isinstance(reference_break_time, (pd.Timestamp, datetime)):\n",
        "            i_fvg = fvg_candidates[\n",
        "                (~fvg_candidates['mitigated']) &\n",
        "                (fvg_candidates.index < reference_break_time)\n",
        "            ]\n",
        "        else:\n",
        "            i_fvg = fvg_candidates[~fvg_candidates['mitigated']]\n",
        "    else:\n",
        "        i_fvg = fvg_candidates[~fvg_candidates['mitigated']]\n",
        "\n",
        "    return i_fvg\n"
      ],
      "metadata": {
        "id": "yTBH6Whh0QqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Order Block"
      ],
      "metadata": {
        "id": "0_TN_Ntd4T38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_order_blocks(df, atr_period=14, atr_mult=0.5, use_atr_filter=False):\n",
        "    \"\"\"\n",
        "    Identify bullish and bearish Order Blocks independently.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Must contain columns ['open', 'high', 'low', 'close'].\n",
        "    atr_period : int, optional\n",
        "        ATR lookback period for displacement filter (default=14).\n",
        "    atr_mult : float, optional\n",
        "        Multiplier for ATR filter (default=0.5).\n",
        "    use_atr_filter : bool, optional\n",
        "        If True, only accept OBs with displacement > ATR * atr_mult.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Original DataFrame with added columns:\n",
        "        - ob_type : int (1 = bullish OB, -1 = bearish OB, 0 = none)\n",
        "        - ob_top : float (high of the OB candle)\n",
        "        - ob_bottom : float (low of the OB candle)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['ob_type'] = 0\n",
        "    df['ob_top'] = np.nan\n",
        "    df['ob_bottom'] = np.nan\n",
        "\n",
        "    # Calculate ATR if filter enabled\n",
        "    if use_atr_filter:\n",
        "        hl_range = df['high'] - df['low']\n",
        "        hc_range = (df['high'] - df['close'].shift()).abs()\n",
        "        lc_range = (df['low'] - df['close'].shift()).abs()\n",
        "        tr = pd.concat([hl_range, hc_range, lc_range], axis=1).max(axis=1)\n",
        "        df['atr'] = tr.rolling(window=atr_period, min_periods=1).mean()\n",
        "    else:\n",
        "        df['atr'] = np.nan\n",
        "\n",
        "    if len(df) < 4:\n",
        "        return df\n",
        "\n",
        "    # Loop through bars to detect OBs\n",
        "    for i in range(3, len(df) - 1):\n",
        "        cur = df.iloc[i]\n",
        "        prev = df.iloc[i - 1]\n",
        "        ob_candle = df.iloc[i - 2]\n",
        "        ob_prev_candle = df.iloc[i - 3]\n",
        "\n",
        "        # Skip if any key candle data is missing\n",
        "        if pd.isna(ob_prev_candle['low']) or pd.isna(ob_prev_candle['high']):\n",
        "            continue\n",
        "\n",
        "        # Price displacement logic\n",
        "        bullish_displacement = (cur['close'] > cur['open']) and (cur['close'] > prev['high'])\n",
        "        bearish_displacement = (cur['close'] < cur['open']) and (cur['close'] < prev['low'])\n",
        "\n",
        "        # ATR filter for displacement\n",
        "        if use_atr_filter:\n",
        "            move_size = abs(cur['close'] - cur['open'])\n",
        "            if move_size < (cur['atr'] * atr_mult):\n",
        "                continue\n",
        "\n",
        "        # Bullish OB: last down candle before strong bullish move & liquidity sweep\n",
        "        if bullish_displacement and (ob_candle['close'] < ob_candle['open']):\n",
        "            swept_liquidity = ob_candle['low'] < ob_prev_candle['low']\n",
        "            if swept_liquidity:\n",
        "                df.at[i - 2, 'ob_type'] = 1\n",
        "                df.at[i - 2, 'ob_top'] = ob_candle['high']\n",
        "                df.at[i - 2, 'ob_bottom'] = ob_candle['low']\n",
        "\n",
        "        # Bearish OB: last up candle before strong bearish move & liquidity sweep\n",
        "        elif bearish_displacement and (ob_candle['close'] > ob_candle['open']):\n",
        "            swept_liquidity = ob_candle['high'] > ob_prev_candle['high']\n",
        "            if swept_liquidity:\n",
        "                df.at[i - 2, 'ob_type'] = -1\n",
        "                df.at[i - 2, 'ob_top'] = ob_candle['high']\n",
        "                df.at[i - 2, 'ob_bottom'] = ob_candle['low']\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "wVx-e8AM4SIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Breaker Block\n",
        "\n",
        "def identify_breaker_blocks(df,\n",
        "                             sweep_lookahead=10,\n",
        "                             break_lookahead=30,\n",
        "                             atr_period=14,\n",
        "                             atr_mult=0.5,\n",
        "                             use_atr_filter=False):\n",
        "    \"\"\"\n",
        "    Identify breaker blocks from price action.\n",
        "\n",
        "    Breaker Block Definition:\n",
        "      - Starts as an Order Block (OB)\n",
        "      - Liquidity sweep (price pierces OB level)\n",
        "      - Then price reverses and breaks structure in the opposite direction.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Must have ['open','high','low','close'].\n",
        "        sweep_lookahead (int): Bars after OB to check for liquidity sweep.\n",
        "        break_lookahead (int): Bars after sweep to check for structure break.\n",
        "        atr_period (int): ATR period for OB filter.\n",
        "        atr_mult (float): Min displacement relative to ATR.\n",
        "        use_atr_filter (bool): If True, filter weak OB moves.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Breaker blocks with columns:\n",
        "                      ['ob_index', 'ob_type', 'ob_top', 'ob_bottom', 'breaker_detected_at']\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # -------------------\n",
        "    # Internal OB detection\n",
        "    # -------------------\n",
        "    df['ob_type'] = 0\n",
        "    df['ob_top'] = np.nan\n",
        "    df['ob_bottom'] = np.nan\n",
        "\n",
        "    if use_atr_filter:\n",
        "        high_low = df['high'] - df['low']\n",
        "        high_close = np.abs(df['high'] - df['close'].shift())\n",
        "        low_close = np.abs(df['low'] - df['close'].shift())\n",
        "        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "        df['atr'] = tr.rolling(window=atr_period, min_periods=1).mean()\n",
        "    else:\n",
        "        df['atr'] = np.nan\n",
        "\n",
        "    for i in range(3, len(df) - 1):\n",
        "        cur = df.iloc[i]\n",
        "        prev = df.iloc[i - 1]\n",
        "        ob = df.iloc[i - 2]\n",
        "        ob_prev = df.iloc[i - 3]\n",
        "\n",
        "        is_bull_move = (cur['close'] > cur['open']) and (cur['close'] > prev['high'])\n",
        "        is_bear_move = (cur['close'] < cur['open']) and (cur['close'] < prev['low'])\n",
        "\n",
        "        if use_atr_filter:\n",
        "            move_size = abs(cur['close'] - cur['open'])\n",
        "            if move_size < (cur['atr'] * atr_mult):\n",
        "                continue\n",
        "\n",
        "        if is_bull_move and ob['close'] < ob['open']:\n",
        "            swept_liq = ob['low'] < ob_prev['low']\n",
        "            if swept_liq:\n",
        "                df.iloc[i - 2, df.columns.get_loc('ob_type')] = 1\n",
        "                df.iloc[i - 2, df.columns.get_loc('ob_top')] = ob['high']\n",
        "                df.iloc[i - 2, df.columns.get_loc('ob_bottom')] = ob['low']\n",
        "\n",
        "        elif is_bear_move and ob['close'] > ob['open']:\n",
        "            swept_liq = ob['high'] > ob_prev['high']\n",
        "            if swept_liq:\n",
        "                df.iloc[i - 2, df.columns.get_loc('ob_type')] = -1\n",
        "                df.iloc[i - 2, df.columns.get_loc('ob_top')] = ob['high']\n",
        "                df.iloc[i - 2, df.columns.get_loc('ob_bottom')] = ob['low']\n",
        "\n",
        "    # -------------------\n",
        "    # Breaker block detection\n",
        "    # -------------------\n",
        "    breakers = []\n",
        "    obs = df[df['ob_type'] != 0]\n",
        "\n",
        "    for idx in obs.index:\n",
        "        loc = df.index.get_loc(idx)\n",
        "        ob_type = obs.loc[idx, 'ob_type']\n",
        "        ob_top = obs.loc[idx, 'ob_top']\n",
        "        ob_bottom = obs.loc[idx, 'ob_bottom']\n",
        "\n",
        "        # Step 1: Liquidity sweep\n",
        "        sweep_found = False\n",
        "        sweep_idx = None\n",
        "        for j in range(loc + 1, min(len(df), loc + 1 + sweep_lookahead)):\n",
        "            bar = df.iloc[j]\n",
        "            if ob_type == 1 and bar['low'] < ob_bottom:  # Bullish OB swept below\n",
        "                sweep_found = True\n",
        "                sweep_idx = j\n",
        "                break\n",
        "            elif ob_type == -1 and bar['high'] > ob_top:  # Bearish OB swept above\n",
        "                sweep_found = True\n",
        "                sweep_idx = j\n",
        "                break\n",
        "\n",
        "        if not sweep_found:\n",
        "            continue\n",
        "\n",
        "        # Step 2: Opposite structure break\n",
        "        for k in range(sweep_idx + 1, min(len(df), sweep_idx + 1 + break_lookahead)):\n",
        "            bar = df.iloc[k]\n",
        "            if ob_type == 1:\n",
        "                recent_high = df['high'].iloc[max(0, k - 10):k].max()\n",
        "                if bar['close'] > recent_high:\n",
        "                    breakers.append({\n",
        "                        'ob_index': idx,\n",
        "                        'ob_type': ob_type,\n",
        "                        'ob_top': ob_top,\n",
        "                        'ob_bottom': ob_bottom,\n",
        "                        'breaker_detected_at': df.index[k]\n",
        "                    })\n",
        "                    break\n",
        "            else:\n",
        "                recent_low = df['low'].iloc[max(0, k - 10):k].min()\n",
        "                if bar['close'] < recent_low:\n",
        "                    breakers.append({\n",
        "                        'ob_index': idx,\n",
        "                        'ob_type': ob_type,\n",
        "                        'ob_top': ob_top,\n",
        "                        'ob_bottom': ob_bottom,\n",
        "                        'breaker_detected_at': df.index[k]\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "    return pd.DataFrame(breakers)\n",
        "\n"
      ],
      "metadata": {
        "id": "w4rMMn0v696S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Htf Poi mitigation\n",
        "def find_htf_poi_with_mitigation(\n",
        "    htf_data,\n",
        "    min_bars=10,\n",
        "    min_penetration=0.0,   # 0.0 = any touch, 1.0 = full body penetration\n",
        "    return_all=False,      # If False, returns latest bullish/bearish only\n",
        "    use_body_only=False    # If True, checks only candle body for mitigation\n",
        "):\n",
        "    \"\"\"\n",
        "    Identify high-timeframe unmitigated order blocks (POIs) with more control.\n",
        "\n",
        "    Args:\n",
        "        htf_data (pd.DataFrame): Must contain open, high, low, close.\n",
        "        min_bars (int): Minimum bars required to process.\n",
        "        min_penetration (float): Min % penetration into OB for mitigation (0-1).\n",
        "        return_all (bool): Return all unmitigated OBs if True.\n",
        "        use_body_only (bool): Check only candle body for mitigation.\n",
        "\n",
        "    Returns:\n",
        "        list of dicts with OB details.\n",
        "    \"\"\"\n",
        "    if htf_data is None or len(htf_data) < min_bars:\n",
        "        return []\n",
        "\n",
        "    # Ensure OBs exist\n",
        "    htf_data = identify_order_blocks(htf_data.copy())\n",
        "    if 'ob_type' not in htf_data.columns:\n",
        "        return []\n",
        "\n",
        "    valid_obs = htf_data[htf_data['ob_type'] != 0].copy()\n",
        "    if valid_obs.empty:\n",
        "        return []\n",
        "\n",
        "    valid_obs['mitigated'] = False\n",
        "\n",
        "    # Vectorized mitigation check\n",
        "    for idx in valid_obs.index:\n",
        "        try:\n",
        "            loc = htf_data.index.get_loc(idx)\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        ob_top = valid_obs.at[idx, 'ob_top']\n",
        "        ob_bottom = valid_obs.at[idx, 'ob_bottom']\n",
        "\n",
        "        # Get the future candles after the OB\n",
        "        future = htf_data.iloc[loc+1:]\n",
        "\n",
        "        if use_body_only:\n",
        "            future_highs = future['close'].where(future['close'] > future['open'], future['open'])\n",
        "            future_lows = future['close'].where(future['close'] < future['open'], future['open'])\n",
        "        else:\n",
        "            future_highs = future['high']\n",
        "            future_lows = future['low']\n",
        "\n",
        "        # Calculate penetration\n",
        "        if valid_obs.at[idx, 'ob_type'] == 1:  # Bullish OB\n",
        "            penetration = (ob_top - future_lows) / (ob_top - ob_bottom)\n",
        "            if (penetration >= min_penetration).any():\n",
        "                valid_obs.at[idx, 'mitigated'] = True\n",
        "        else:  # Bearish OB\n",
        "            penetration = (future_highs - ob_bottom) / (ob_top - ob_bottom)\n",
        "            if (penetration >= min_penetration).any():\n",
        "                valid_obs.at[idx, 'mitigated'] = True\n",
        "\n",
        "    # Keep unmitigated OBs\n",
        "    unmit = valid_obs[~valid_obs['mitigated']].copy()\n",
        "\n",
        "    # Add metadata\n",
        "    unmit['duration_bars'] = len(htf_data) - htf_data.index.get_indexer(unmit.index)\n",
        "\n",
        "    if return_all:\n",
        "        return unmit.to_dict('records')\n",
        "\n",
        "    pois = []\n",
        "    if not unmit[unmit['ob_type'] == 1].empty:\n",
        "        pois.append(unmit[unmit['ob_type'] == 1].iloc[-1].to_dict())\n",
        "    if not unmit[unmit['ob_type'] == -1].empty:\n",
        "        pois.append(unmit[unmit['ob_type'] == -1].iloc[-1].to_dict())\n",
        "\n",
        "    return pois\n"
      ],
      "metadata": {
        "id": "KNDGo9SD8Igv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HTF Analysis\n"
      ],
      "metadata": {
        "id": "V0e3BUIY7-Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_valid_pullback_idm(\n",
        "    df,\n",
        "    swing_candle_index,\n",
        "    lookback_limit=50,\n",
        "    is_bullish_trend=True,\n",
        "    htf_bias=None,\n",
        "    use_swing_points=True,\n",
        "    min_atr_mult=0.5,\n",
        "    use_bodies=False,\n",
        "    active_sessions=None  # e.g., [('09:30','16:00'), ('20:00','23:00')]\n",
        "):\n",
        "    \"\"\"\n",
        "    Find a valid pullback IDM level with improved filtering.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with 'high', 'low', 'open', 'close', 'atr', optional 'is_swing_high/low'.\n",
        "    - swing_candle_index: Index of the swing point to start searching from.\n",
        "    - lookback_limit: Max candles to look back.\n",
        "    - is_bullish_trend: True if uptrend, False if downtrend.\n",
        "    - htf_bias: 'bullish', 'bearish', or None → filters to match HTF direction.\n",
        "    - use_swing_points: If True, only consider pullbacks at swing points.\n",
        "    - min_atr_mult: Minimum ATR multiple for pullback displacement.\n",
        "    - use_bodies: If True, use candle bodies instead of wicks for detection.\n",
        "    - active_sessions: List of (start_time, end_time) tuples in HH:MM format to filter valid pullbacks.\n",
        "\n",
        "    Returns:\n",
        "    - pullback_level, pullback_index\n",
        "    \"\"\"\n",
        "    try:\n",
        "        swing_loc = df.index.get_loc(swing_candle_index)\n",
        "    except KeyError:\n",
        "        return np.nan, None\n",
        "    if swing_loc < 1:\n",
        "        return np.nan, None\n",
        "\n",
        "    # Check HTF bias filter\n",
        "    if htf_bias is not None:\n",
        "        if htf_bias == 'bullish' and not is_bullish_trend:\n",
        "            return np.nan, None\n",
        "        if htf_bias == 'bearish' and is_bullish_trend:\n",
        "            return np.nan, None\n",
        "\n",
        "    limit_loc = max(0, swing_loc - lookback_limit)\n",
        "\n",
        "    for i in range(swing_loc - 1, limit_loc - 1, -1):\n",
        "        cur = df.iloc[i]\n",
        "        prev = df.iloc[i - 1]\n",
        "\n",
        "        # Optional: filter by active session\n",
        "        if active_sessions:\n",
        "            candle_time = pd.to_datetime(df.index[i]).time()\n",
        "            if not any(start <= candle_time.strftime(\"%H:%M\") <= end for start, end in active_sessions):\n",
        "                continue\n",
        "\n",
        "        # Swing point filter\n",
        "        if use_swing_points:\n",
        "            if is_bullish_trend and not cur.get('is_swing_low', False):\n",
        "                continue\n",
        "            if not is_bullish_trend and not cur.get('is_swing_high', False):\n",
        "                continue\n",
        "\n",
        "        # ATR filter\n",
        "        if 'atr' in df.columns:\n",
        "            move_size = abs(cur['close'] - prev['close'])\n",
        "            if move_size < (cur['atr'] * min_atr_mult):\n",
        "                continue\n",
        "\n",
        "        # Wick/body detection\n",
        "        if is_bullish_trend:\n",
        "            val = cur['low'] if not use_bodies else min(cur['open'], cur['close'])\n",
        "            prev_val = prev['low'] if not use_bodies else min(prev['open'], prev['close'])\n",
        "            if val < prev_val:\n",
        "                return val, df.index[i]\n",
        "        else:\n",
        "            val = cur['high'] if not use_bodies else max(cur['open'], cur['close'])\n",
        "            prev_val = prev['high'] if not use_bodies else max(prev['open'], prev['close'])\n",
        "            if val > prev_val:\n",
        "                return val, df.index[i]\n",
        "\n",
        "    return np.nan, None\n",
        "\n",
        "def update_market_structure(\n",
        "    df,\n",
        "    market_state,\n",
        "    swing_order=3,\n",
        "    bos_break_type='close',  # 'close' or 'wick'\n",
        "    min_bos_displacement=0.0,  # in price units or ATR multiples\n",
        "    check_liquidity_sweep=False,\n",
        "    htf_bias=None,\n",
        "    plot=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Update market structure with BOS/CHOCH & IDM tracking.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with OHLC + swing points + ATR + optional liquidity markers.\n",
        "    - market_state: dict for persistent HTF state (passed in/out, no globals).\n",
        "    - swing_order: Swing point order for detection.\n",
        "    - bos_break_type: 'close' = close must break, 'wick' = high/low break.\n",
        "    - min_bos_displacement: Minimum displacement to count as BOS.\n",
        "    - check_liquidity_sweep: Require liquidity sweep before BOS.\n",
        "    - htf_bias: 'bullish', 'bearish', or None to filter.\n",
        "    - plot: If True, annotate df with rectangles/labels.\n",
        "\n",
        "    Returns:\n",
        "    - df: Updated DataFrame with BOS/CHOCH columns.\n",
        "    - market_state: Updated state dict.\n",
        "    - events: List of BOS/CHOCH events.\n",
        "    \"\"\"\n",
        "    events = []\n",
        "\n",
        "    if df is None or len(df) < swing_order * 2 + 1:\n",
        "        return df, market_state, events\n",
        "\n",
        "    df = identify_swing_points(df.copy(), order=swing_order)\n",
        "\n",
        "    swing_highs = df[df['is_swing_high']]\n",
        "    swing_lows = df[df['is_swing_low']]\n",
        "    if swing_highs.empty or swing_lows.empty:\n",
        "        return df, market_state, events\n",
        "\n",
        "    last_sh_index = swing_highs.index[-1]\n",
        "    last_sl_index = swing_lows.index[-1]\n",
        "    last_sh_val = swing_highs['high'].iloc[-1]\n",
        "    last_sl_val = swing_lows['low'].iloc[-1]\n",
        "\n",
        "    # Determine trend context\n",
        "    if last_sh_index > last_sl_index:\n",
        "        new_idm_level, new_idm_time_index = find_valid_pullback_idm(\n",
        "            df, last_sh_index, 50, True, htf_bias=htf_bias\n",
        "        )\n",
        "        idm_trend_context = 1\n",
        "    else:\n",
        "        new_idm_level, new_idm_time_index = find_valid_pullback_idm(\n",
        "            df, last_sl_index, 50, False, htf_bias=htf_bias\n",
        "        )\n",
        "        idm_trend_context = -1\n",
        "\n",
        "    # Store IDM if new\n",
        "    if new_idm_time_index is not None and (\n",
        "        market_state[\"idm_time_index\"] is None or new_idm_time_index > market_state[\"idm_time_index\"]\n",
        "    ):\n",
        "        market_state[\"idm_level\"] = new_idm_level\n",
        "        market_state[\"idm_time_index\"] = new_idm_time_index\n",
        "        market_state[\"idm_taken\"] = False\n",
        "\n",
        "    # Check IDM taken\n",
        "    if not market_state[\"idm_taken\"] and market_state[\"idm_time_index\"] is not None:\n",
        "        after = df[df.index > market_state[\"idm_time_index\"]]\n",
        "        if not after.empty:\n",
        "            if idm_trend_context == 1 and (after['low'] < market_state[\"idm_level\"]).any():\n",
        "                market_state[\"idm_taken\"] = True\n",
        "            elif idm_trend_context == -1 and (after['high'] > market_state[\"idm_level\"]).any():\n",
        "                market_state[\"idm_taken\"] = True\n",
        "\n",
        "    # BOS/CHOCH detection\n",
        "    df['BOS'] = 0\n",
        "    df['CHOCH'] = 0\n",
        "    start_check_index = market_state.get(\"last_bos_choch_time_index\", df.index[0])\n",
        "    candles_to_check = df[df.index > start_check_index]\n",
        "\n",
        "    for current_index, current_candle in candles_to_check.iterrows():\n",
        "        for direction in [1, -1]:\n",
        "            if direction == 1 and not np.isnan(market_state.get(\"last_confirmed_high\", np.nan)):\n",
        "                trigger_price = (\n",
        "                    market_state[\"last_confirmed_high\"] if bos_break_type == 'wick' else current_candle['close']\n",
        "                )\n",
        "                if trigger_price > market_state[\"last_confirmed_high\"]:\n",
        "                    if check_liquidity_sweep and not current_candle.get('liq_sweep_high', False):\n",
        "                        continue\n",
        "                    events.append((current_index, 'BOS' if market_state[\"trend\"] == 1 else 'CHOCH', direction))\n",
        "                    df.loc[current_index, 'BOS' if market_state[\"trend\"] == 1 else 'CHOCH'] = direction\n",
        "                    market_state[\"trend\"] = 1\n",
        "                    market_state.update({\n",
        "                        \"last_confirmed_low\": market_state[\"last_confirmed_high\"],\n",
        "                        \"last_low_index\": market_state[\"last_high_index\"],\n",
        "                        \"last_confirmed_high\": np.nan,\n",
        "                        \"last_high_index\": None,\n",
        "                        \"idm_taken\": False,\n",
        "                        \"idm_level\": np.nan,\n",
        "                        \"idm_time_index\": None,\n",
        "                        \"last_bos_choch_time_index\": current_index\n",
        "                    })\n",
        "            elif direction == -1 and not np.isnan(market_state.get(\"last_confirmed_low\", np.nan)):\n",
        "                trigger_price = (\n",
        "                    market_state[\"last_confirmed_low\"] if bos_break_type == 'wick' else current_candle['close']\n",
        "                )\n",
        "                if trigger_price < market_state[\"last_confirmed_low\"]:\n",
        "                    if check_liquidity_sweep and not current_candle.get('liq_sweep_low', False):\n",
        "                        continue\n",
        "                    events.append((current_index, 'BOS' if market_state[\"trend\"] == -1 else 'CHOCH', direction))\n",
        "                    df.loc[current_index, 'BOS' if market_state[\"trend\"] == -1 else 'CHOCH'] = direction\n",
        "                    market_state[\"trend\"] = -1\n",
        "                    market_state.update({\n",
        "                        \"last_confirmed_high\": market_state[\"last_confirmed_low\"],\n",
        "                        \"last_high_index\": market_state[\"last_low_index\"],\n",
        "                        \"last_confirmed_low\": np.nan,\n",
        "                        \"last_low_index\": None,\n",
        "                        \"idm_taken\": False,\n",
        "                        \"idm_level\": np.nan,\n",
        "                        \"idm_time_index\": None,\n",
        "                        \"last_bos_choch_time_index\": current_index\n",
        "                    })\n",
        "\n",
        "    if plot:\n",
        "        # Optional: plotting logic for BOS/CHOCH + IDM\n",
        "        pass\n",
        "\n",
        "    return df, market_state, events\n"
      ],
      "metadata": {
        "id": "J-ip9zs_8LpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LTF Detection"
      ],
      "metadata": {
        "id": "wpYTFCPi-OQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_ltf_structure(ltf_df, pct=0.004, use_atr=False, atr_period=14):\n",
        "    \"\"\"\n",
        "    Map LTF market structure using zigzag-based swing highs/lows.\n",
        "\n",
        "    Parameters:\n",
        "        ltf_df (pd.DataFrame): OHLC dataframe with datetime index.\n",
        "        pct (float): Zigzag percentage threshold for swing detection.\n",
        "        use_atr (bool): If True, ATR is used for dynamic thresholds.\n",
        "        atr_period (int): ATR period if use_atr=True.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Original DF with 'ltf_bos' and 'ltf_choch' columns.\n",
        "    \"\"\"\n",
        "    # Copy to avoid modifying original\n",
        "    ltf_df = ltf_df.copy()\n",
        "    ltf_df['ltf_bos'] = 0\n",
        "    ltf_df['ltf_choch'] = 0\n",
        "\n",
        "    # --- Detect swing highs/lows ---\n",
        "    pivots = zigzag_pivots_from_df(\n",
        "        ltf_df, pct=pct, use_atr=use_atr, atr_period=atr_period\n",
        "    )\n",
        "\n",
        "    swing_highs = pd.DataFrame(\n",
        "        [(ts, price) for ts, price, typ in pivots if typ == 'high'],\n",
        "        columns=['ts', 'price']\n",
        "    ).set_index('ts')\n",
        "\n",
        "    swing_lows = pd.DataFrame(\n",
        "        [(ts, price) for ts, price, typ in pivots if typ == 'low'],\n",
        "        columns=['ts', 'price']\n",
        "    ).set_index('ts')\n",
        "\n",
        "    last_sh = swing_highs['price'].iloc[-1] if not swing_highs.empty else np.nan\n",
        "    last_sl = swing_lows['price'].iloc[-1] if not swing_lows.empty else np.nan\n",
        "    trend = 0  # 1 = bullish, -1 = bearish\n",
        "\n",
        "    # --- Trend initialization ---\n",
        "    if not swing_highs.empty and not swing_lows.empty:\n",
        "        if swing_highs.index[-1] > swing_lows.index[-1] and len(swing_highs) > 1:\n",
        "            if last_sh > swing_highs['price'].iloc[-2]:\n",
        "                if len(swing_lows) > 1 and last_sl > swing_lows['price'].iloc[-2]:\n",
        "                    trend = 1\n",
        "        elif swing_lows.index[-1] > swing_highs.index[-1] and len(swing_lows) > 1:\n",
        "            if last_sl < swing_lows['price'].iloc[-2]:\n",
        "                if len(swing_highs) > 1 and last_sh < swing_highs['price'].iloc[-2]:\n",
        "                    trend = -1\n",
        "\n",
        "    # --- Loop through bars ---\n",
        "    for i in range(1, len(ltf_df)):\n",
        "        idx = ltf_df.index[i]\n",
        "        close = ltf_df['close'].iloc[i]\n",
        "\n",
        "        # Bullish break\n",
        "        if not np.isnan(last_sh) and close > last_sh:\n",
        "            if trend == 1:\n",
        "                ltf_df.loc[idx, 'ltf_bos'] = 1\n",
        "            else:\n",
        "                ltf_df.loc[idx, 'ltf_choch'] = 1\n",
        "            trend = 1\n",
        "            # Update last swing low\n",
        "            lows_before = swing_lows[swing_lows.index < idx]\n",
        "            if not lows_before.empty:\n",
        "                last_sl = lows_before['price'].iloc[-1]\n",
        "            last_sh = ltf_df['high'].iloc[i]\n",
        "\n",
        "        # Bearish break\n",
        "        elif not np.isnan(last_sl) and close < last_sl:\n",
        "            if trend == -1:\n",
        "                ltf_df.loc[idx, 'ltf_bos'] = -1\n",
        "            else:\n",
        "                ltf_df.loc[idx, 'ltf_choch'] = -1\n",
        "            trend = -1\n",
        "            # Update last swing high\n",
        "            highs_before = swing_highs[swing_highs.index < idx]\n",
        "            if not highs_before.empty:\n",
        "                last_sh = highs_before['price'].iloc[-1]\n",
        "            last_sl = ltf_df['low'].iloc[i]\n",
        "\n",
        "    return ltf_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_ltf_idm_after_choch(\n",
        "    ltf_df, choch_index, is_bullish_choch,\n",
        "    look_ahead_bars=10, min_displacement=0.0, break_check_bars=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Finds the first valid IDM level after a CHOCH in LTF data.\n",
        "\n",
        "    Parameters:\n",
        "        ltf_df: DataFrame with 'high', 'low' columns.\n",
        "        choch_index: Index of CHOCH candle.\n",
        "        is_bullish_choch: True for bullish CHOCH, False for bearish.\n",
        "        look_ahead_bars: How many candles ahead to search for IDM.\n",
        "        min_displacement: Minimum price move to qualify as IDM.\n",
        "        break_check_bars: How many candles to check for IDM being taken.\n",
        "\n",
        "    Returns:\n",
        "        (idm_level, idm_time_index, idm_taken)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        choch_loc = ltf_df.index.get_loc(choch_index)\n",
        "    except KeyError:\n",
        "        return np.nan, None, False\n",
        "\n",
        "    idm_level = np.nan\n",
        "    idm_time_index = None\n",
        "    idm_taken = False\n",
        "\n",
        "    search_end = min(len(ltf_df), choch_loc + look_ahead_bars + 1)\n",
        "\n",
        "    for i in range(choch_loc + 1, search_end):\n",
        "        cur = ltf_df.iloc[i]\n",
        "        prev = ltf_df.iloc[i - 1]\n",
        "\n",
        "        if is_bullish_choch and cur['low'] < prev['low'] - min_displacement:\n",
        "            idm_level = cur['low']\n",
        "            idm_time_index = ltf_df.index[i]\n",
        "            break\n",
        "        elif not is_bullish_choch and cur['high'] > prev['high'] + min_displacement:\n",
        "            idm_level = cur['high']\n",
        "            idm_time_index = ltf_df.index[i]\n",
        "            break\n",
        "\n",
        "    # Check if IDM is taken within allowed bars\n",
        "    if idm_time_index:\n",
        "        end_check = min(len(ltf_df), ltf_df.index.get_loc(idm_time_index) + break_check_bars + 1)\n",
        "        after_idm = ltf_df.iloc[ltf_df.index.get_loc(idm_time_index) + 1:end_check]\n",
        "        if is_bullish_choch and (after_idm['low'] < idm_level).any():\n",
        "            idm_taken = True\n",
        "        elif not is_bullish_choch and (after_idm['high'] > idm_level).any():\n",
        "            idm_taken = True\n",
        "\n",
        "    return idm_level, idm_time_index, idm_taken\n",
        "\n",
        "\n",
        "\n",
        "def find_entry_zone_after_idm(ltf_df, idm_taken_index, is_bullish_entry):\n",
        "    \"\"\"\n",
        "    Find the first valid Order Block (OB) or Fair Value Gap (FVG)\n",
        "    after an IDM has been taken out.\n",
        "\n",
        "    ltf_df: DataFrame with OHLC and liquidity structure columns.\n",
        "    idm_taken_index: index (timestamp) where IDM was taken.\n",
        "    is_bullish_entry: True for bullish setups, False for bearish.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series of the chosen OB/FVG candle, or None if not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        idm_loc = ltf_df.index.get_loc(idm_taken_index)\n",
        "    except KeyError:\n",
        "        return None\n",
        "\n",
        "    # Ensure OB & FVG columns are present\n",
        "    ltf_df = identify_order_blocks(ltf_df.copy())\n",
        "\n",
        "    for i in range(idm_loc, len(ltf_df)):\n",
        "        candle = ltf_df.iloc[i]\n",
        "        prev_candle = ltf_df.iloc[i-1] if i > 0 else None\n",
        "\n",
        "        # ----------- Bullish case -----------\n",
        "        if is_bullish_entry:\n",
        "            # ✅ Order Block check\n",
        "            if candle['ob_type'] == 1:\n",
        "                if i + 1 < len(ltf_df) and not (ltf_df['low'].iloc[i+1:] < candle['ob_bottom']).any():\n",
        "                    return candle\n",
        "            # ✅ Fair Value Gap check\n",
        "            if prev_candle is not None and prev_candle['fvg_type'] == 1:\n",
        "                if not (ltf_df['low'].iloc[i:] < prev_candle['fvg_bottom']).any():\n",
        "                    return prev_candle\n",
        "\n",
        "        # ----------- Bearish case -----------\n",
        "        else:\n",
        "            # ✅ Order Block check\n",
        "            if candle['ob_type'] == -1:\n",
        "                if i + 1 < len(ltf_df) and not (ltf_df['high'].iloc[i+1:] > candle['ob_top']).any():\n",
        "                    return candle\n",
        "            # ✅ Fair Value Gap check\n",
        "            if prev_candle is not None and prev_candle['fvg_type'] == -1:\n",
        "                if not (ltf_df['high'].iloc[i:] > prev_candle['fvg_top']).any():\n",
        "                    return prev_candle\n",
        "\n",
        "    # No valid zone found\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "JLgHwA_N-SdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equilibrium"
      ],
      "metadata": {
        "id": "Wz_TBIHHBPAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_equilibrium_between_swings(df, high_index, low_index):\n",
        "    \"\"\"\n",
        "    Returns the midpoint price between swing high and swing low (equilibrium).\n",
        "    Accepts indices (timestamps) for the swing high and low.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        high_val = df.loc[high_index]['high']\n",
        "        low_val = df.loc[low_index]['low']\n",
        "    except Exception:\n",
        "        return None\n",
        "    return (high_val + low_val) / 2.0\n",
        "\n",
        "def is_entry_in_discount_or_premium(entry_price, equilibrium_price, signal_type):\n",
        "    \"\"\"\n",
        "    For BUY require entry < equilibrium (discount).\n",
        "    For SELL require entry > equilibrium (premium).\n",
        "    \"\"\"\n",
        "    if equilibrium_price is None:\n",
        "        return True\n",
        "    if signal_type == 'BUY':\n",
        "        return entry_price < equilibrium_price\n",
        "    else:\n",
        "        return entry_price > equilibrium_price"
      ],
      "metadata": {
        "id": "faRi1cTKBOH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LTF ifvg and breakers"
      ],
      "metadata": {
        "id": "T05udgrdDzK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LTF Ifvg and breaker blocks\n",
        "\n",
        "def check_ltf_confirmation_advanced(ltf_data, poi, signal_type):\n",
        "    \"\"\"\n",
        "    Returns (confirmation_type, entry_price, sl_price, confluences_list)\n",
        "    New confluences: 'sweep', 'choch_idm', 'flip', 'ifvg', 'breaker', 'ob'\n",
        "    Equilibrium filter applied when possible.\n",
        "    \"\"\"\n",
        "    if ltf_data is None or len(ltf_data) < 30:\n",
        "        return None, None, None, []\n",
        "    poi_top = poi['ob_top']; poi_bottom = poi['ob_bottom']\n",
        "    poi_size = poi_top - poi_bottom if (not pd.isna(poi_top) and not pd.isna(poi_bottom)) else 0.0\n",
        "\n",
        "    # Find mitigation start\n",
        "    mitigation_start_index = None; mitigation_loc = -1\n",
        "    check_range = min(60, len(ltf_data))\n",
        "    for i in range(len(ltf_data)-1, len(ltf_data)-check_range-1, -1):\n",
        "        candle = ltf_data.iloc[i]\n",
        "        if not (pd.isna(candle['low']) or pd.isna(candle['high'])):\n",
        "            if max(candle['low'], poi_bottom) <= min(candle['high'], poi_top):\n",
        "                mitigation_start_index = ltf_data.index[i]; mitigation_loc = i\n",
        "                break\n",
        "    if mitigation_start_index is None:\n",
        "        return None, None, None, []\n",
        "\n",
        "    ltf_since = ltf_data.iloc[mitigation_loc:].copy()\n",
        "    if len(ltf_since) < 5:\n",
        "        return None, None, None, []\n",
        "\n",
        "    # compute ATR for buffer\n",
        "    atr_series = atr(ltf_data, 14)\n",
        "    atr_since = atr_series.loc[ltf_since.index] if atr_series is not None else None\n",
        "\n",
        "    confluences = []\n",
        "    entry_price = None; sl_price = None; confirmation = None\n",
        "\n",
        "    # Sweep detection (improved)\n",
        "    sweep_found = False; sweep_entry=None; sweep_sl=None\n",
        "    lookback_n = 8\n",
        "    recent_lows_min = ltf_since['low'].iloc[max(0, len(ltf_since)-lookback_n):].min()\n",
        "    recent_highs_max = ltf_since['high'].iloc[max(0, len(ltf_since)-lookback_n):].max()\n",
        "    for i in range(1, len(ltf_since)):\n",
        "        cur = ltf_since.iloc[i]; prev = ltf_since.iloc[i-1]\n",
        "        in_poi = max(cur['low'], poi_bottom) <= min(cur['high'], poi_top)\n",
        "        if signal_type == 'BUY' and in_poi:\n",
        "            if cur['low'] < recent_lows_min:\n",
        "                if cur['close'] > cur['open'] and cur['close'] > (cur['open'] + cur['low'])/2:\n",
        "                    sweep_found=True; sweep_entry = cur['close']; sweep_sl = cur['low'] - max(abs(poi_size)*SL_BUFFER_PCT, 0.00001); break\n",
        "        if signal_type == 'SELL' and in_poi:\n",
        "            if cur['high'] > recent_highs_max:\n",
        "                if cur['close'] < cur['open'] and cur['close'] < (cur['open'] + cur['high'])/2:\n",
        "                    sweep_found=True; sweep_entry = cur['close']; sweep_sl = cur['high'] + max(abs(poi_size)*SL_BUFFER_PCT, 0.00001); break\n",
        "    if sweep_found: confluences.append('sweep')\n",
        "\n",
        "    # CHOCH + IDM detection (like earlier)\n",
        "    ltf_struct_df = map_ltf_structure(ltf_since)\n",
        "    choch_signals = ltf_struct_df[ltf_struct_df['ltf_choch'] != 0]\n",
        "    choch_found=False; choch_entry=None; choch_sl=None\n",
        "    if not choch_signals.empty:\n",
        "        first_choch = choch_signals.iloc[0]; choch_index = choch_signals.index[0]; choch_type = int(first_choch['ltf_choch'])\n",
        "        if (signal_type=='BUY' and choch_type==1) or (signal_type=='SELL' and choch_type==-1):\n",
        "            if atr_since is not None and choch_index in atr_since.index:\n",
        "                choch_atr = atr_since.loc[choch_index]\n",
        "            else:\n",
        "                choch_atr = max(0.0005, (ltf_since['high']-ltf_since['low']).median())\n",
        "            if choch_type == 1:\n",
        "                broken_level = first_choch['high'] if 'high' in first_choch.index else None\n",
        "                buffer = choch_atr * CHOCH_ATR_BUFFER\n",
        "                if broken_level is not None and (first_choch['close'] - broken_level) > buffer:\n",
        "                    idm_level, idm_time_index, idm_taken = find_ltf_idm_after_choch(ltf_struct_df, choch_index, True)\n",
        "                    if idm_time_index is not None and idm_taken:\n",
        "                        entry_zone = find_entry_zone_after_idm(ltf_struct_df, idm_time_index, True)\n",
        "                        if entry_zone is not None:\n",
        "                            choch_found=True\n",
        "                            if not pd.isna(entry_zone['ob_type']):\n",
        "                                choch_entry = entry_zone['ob_top']; choch_sl = entry_zone['ob_bottom'] - abs(entry_zone['ob_top']-entry_zone['ob_bottom'])*SL_BUFFER_PCT\n",
        "                            elif not pd.isna(entry_zone['fvg_type']):\n",
        "                                choch_entry = entry_zone['fvg_top']; choch_sl = entry_zone['fvg_bottom'] - abs(entry_zone['fvg_top']-entry_zone['fvg_bottom'])*SL_BUFFER_PCT\n",
        "            else:\n",
        "                broken_level = first_choch['low'] if 'low' in first_choch.index else None\n",
        "                buffer = choch_atr * CHOCH_ATR_BUFFER\n",
        "                if broken_level is not None and (broken_level - first_choch['close']) > buffer:\n",
        "                    idm_level, idm_time_index, idm_taken = find_ltf_idm_after_choch(ltf_struct_df, choch_index, False)\n",
        "                    if idm_time_index is not None and idm_taken:\n",
        "                        entry_zone = find_entry_zone_after_idm(ltf_struct_df, idm_time_index, False)\n",
        "                        if entry_zone is not None:\n",
        "                            choch_found=True\n",
        "                            if not pd.isna(entry_zone['ob_type']):\n",
        "                                choch_entry = entry_zone['ob_bottom']; choch_sl = entry_zone['ob_top'] + abs(entry_zone['ob_top']-entry_zone['ob_bottom'])*SL_BUFFER_PCT\n",
        "                            elif not pd.isna(entry_zone['fvg_type']):\n",
        "                                choch_entry = entry_zone['fvg_bottom']; choch_sl = entry_zone['fvg_top'] + abs(entry_zone['fvg_top']-entry_zone['fvg_bottom'])*SL_BUFFER_PCT\n",
        "    if choch_found: confluences.append('choch_idm')\n",
        "\n",
        "    # Flip detection\n",
        "    flip_found=False; flip_entry=None; flip_sl=None\n",
        "    ltf_obs = identify_order_blocks(ltf_since.copy())\n",
        "    for i in range(1, len(ltf_obs)):\n",
        "        ob_prev = ltf_obs.iloc[i-1]\n",
        "        if signal_type=='BUY':\n",
        "            if ob_prev['ob_type']==-1 and max(ob_prev['low'], poi_bottom) <= min(ob_prev['high'], poi_top):\n",
        "                if (ltf_obs['close'].iloc[i:] > ob_prev['ob_top']).any():\n",
        "                    subsequent_bull = ltf_obs[(ltf_obs.index > ltf_obs.index[i-1]) & (ltf_obs['ob_type']==1)]\n",
        "                    if not subsequent_bull.empty:\n",
        "                        flip_found=True\n",
        "                        flip_entry = subsequent_bull.iloc[0]['ob_top']\n",
        "                        flip_sl = subsequent_bull.iloc[0]['ob_bottom'] - abs(subsequent_bull.iloc[0]['ob_top']-subsequent_bull.iloc[0]['ob_bottom'])*SL_BUFFER_PCT\n",
        "                        break\n",
        "        else:\n",
        "            if ob_prev['ob_type']==1 and max(ob_prev['low'], poi_bottom) <= min(ob_prev['high'], poi_top):\n",
        "                if (ltf_obs['close'].iloc[i:] < ob_prev['ob_bottom']).any():\n",
        "                    subsequent_bear = ltf_obs[(ltf_obs.index > ltf_obs.index[i-1]) & (ltf_obs['ob_type']==-1)]\n",
        "                    if not subsequent_bear.empty:\n",
        "                        flip_found=True\n",
        "                        flip_entry = subsequent_bear.iloc[0]['ob_bottom']\n",
        "                        flip_sl = subsequent_bear.iloc[0]['ob_top'] + abs(subsequent_bear.iloc[0]['ob_top']-subsequent_bear.iloc[0]['ob_bottom'])*SL_BUFFER_PCT\n",
        "                        break\n",
        "    if flip_found: confluences.append('flip')\n",
        "\n",
        "    # iFVG detection: use HTF or LTF unmitigated FVGs created before the last HTF break time (if available)\n",
        "    ifvg_found=False; ifvg_entry=None; ifvg_sl=None\n",
        "    # Use the global HTF last_bos_choch_time_index as possible reference\n",
        "    ref_break_time = market_state_htf.get(\"last_bos_choch_time_index\", None)\n",
        "    i_fvgs = identify_ifvg(ltf_data, reference_break_time=ref_break_time)\n",
        "    # choose iFVG inside POI (or close to it)\n",
        "    if not i_fvgs.empty:\n",
        "        for idx,row in i_fvgs.iterrows():\n",
        "            # check if iFVG is near POI (overlapping or adjacent)\n",
        "            if not (pd.isna(row['fvg_bottom']) or pd.isna(row['fvg_top'])):\n",
        "                overlap = max(row['fvg_bottom'], poi_bottom) <= min(row['fvg_top'], poi_top)\n",
        "                if overlap:\n",
        "                    if row['fvg_type'] == 1 and signal_type == 'BUY':\n",
        "                        ifvg_found=True; ifvg_entry = row['fvg_top']; ifvg_sl = row['fvg_bottom'] - abs(row['fvg_top']-row['fvg_bottom'])*SL_BUFFER_PCT; break\n",
        "                    if row['fvg_type'] == -1 and signal_type == 'SELL':\n",
        "                        ifvg_found=True; ifvg_entry = row['fvg_bottom']; ifvg_sl = row['fvg_top'] + abs(row['fvg_top']-row['fvg_bottom'])*SL_BUFFER_PCT; break\n",
        "    if ifvg_found: confluences.append('ifvg')\n",
        "\n",
        "    # Breaker block detection\n",
        "    breaker_found=False; breaker_entry=None; breaker_sl=None\n",
        "    breakers = identify_breaker_blocks(ltf_data)\n",
        "    if breakers:\n",
        "        # pick most recent breaker inside or near POI\n",
        "        for b in reversed(breakers):\n",
        "            b_top = b['ob_top']; b_bottom = b['ob_bottom']\n",
        "            overlap = max(b_bottom, poi_bottom) <= min(b_top, poi_top)\n",
        "            if overlap:\n",
        "                if b['ob_type'] == 1 and signal_type == 'BUY':\n",
        "                    breaker_found=True; breaker_entry = b_top; breaker_sl = b_bottom - abs(b_top-b_bottom)*SL_BUFFER_PCT; break\n",
        "                if b['ob_type'] == -1 and signal_type == 'SELL':\n",
        "                    breaker_found=True; breaker_entry = b_bottom; breaker_sl = b_top + abs(b_top-b_bottom)*SL_BUFFER_PCT; break\n",
        "    if breaker_found: confluences.append('breaker')\n",
        "\n",
        "    # OB present in entry zone is already captured via choch logic but we still add if entry_zone was OB\n",
        "    # Compose final confluences set\n",
        "    unique_confs = list(set(confluences))\n",
        "\n",
        "    # require MIN_CONFLUENCES\n",
        "    if len(unique_confs) < MIN_CONFLUENCES:\n",
        "        return None, None, None, unique_confs\n",
        "\n",
        "    # Equilibrium check: try to compute between last confirmed HTF swings (if available)\n",
        "    equilibrium_price = None\n",
        "    try:\n",
        "        high_idx = market_state_htf.get(\"last_high_index\", None)\n",
        "        low_idx = market_state_htf.get(\"last_low_index\", None)\n",
        "        # Use HTF cache if exists\n",
        "        htf_df = _cache.get(\"htf_df\", None)\n",
        "        if high_idx is not None and low_idx is not None and htf_df is not None:\n",
        "            equilibrium_price = compute_equilibrium_between_swings(htf_df, high_idx, low_idx)\n",
        "    except Exception:\n",
        "        equilibrium_price = None\n",
        "\n",
        "    # Choose final priority: prefer choch_idm + (ifvg|breaker|sweep) etc.\n",
        "    # Candidate mapping\n",
        "    candidate = None\n",
        "    candidate_sl = None\n",
        "    candidate_conf = None\n",
        "    priority = ['choch_idm','ifvg','breaker','sweep','flip']\n",
        "    # if choch_idm exists, try to use its entry\n",
        "    if 'choch_idm' in unique_confs and choch_entry is not None and choch_sl is not None:\n",
        "        candidate, candidate_sl, candidate_conf = choch_entry, choch_sl, 'choch_idm'\n",
        "    elif 'ifvg' in unique_confs and ifvg_entry is not None and ifvg_sl is not None:\n",
        "        candidate, candidate_sl, candidate_conf = ifvg_entry, ifvg_sl, 'ifvg'\n",
        "    elif 'breaker' in unique_confs and breaker_entry is not None and breaker_sl is not None:\n",
        "        candidate, candidate_sl, candidate_conf = breaker_entry, breaker_sl, 'breaker'\n",
        "    elif 'sweep' in unique_confs and sweep_entry is not None and sweep_sl is not None:\n",
        "        candidate, candidate_sl, candidate_conf = sweep_entry, sweep_sl, 'sweep'\n",
        "    elif 'flip' in unique_confs and flip_entry is not None and flip_sl is not None:\n",
        "        candidate, candidate_sl, candidate_conf = flip_entry, flip_sl, 'flip'\n",
        "    else:\n",
        "        # fallback pick whichever exists\n",
        "        for e,s in [('choch',choch_entry),('ifvg',ifvg_entry),('breaker',breaker_entry),('sweep',sweep_entry),('flip',flip_entry)]:\n",
        "            if s is not None:\n",
        "                candidate = s; candidate_conf = e; break\n",
        "\n",
        "    if candidate is None:\n",
        "        return None, None, None, unique_confs\n",
        "\n",
        "    # Equilibrium filter: ensure entry is on discount/premium side\n",
        "    if equilibrium_price is not None:\n",
        "        if not is_entry_in_discount_or_premium(candidate, equilibrium_price, signal_type):\n",
        "            logging.info(f\"Rejected entry by Equilibrium filter: entry {candidate} eq {equilibrium_price} type {signal_type}\")\n",
        "            return None, None, None, unique_confs\n",
        "\n",
        "    # final sanity checks\n",
        "    if signal_type == 'BUY' and candidate_sl >= candidate:\n",
        "        logging.warning(\"Invalid SL for BUY.\")\n",
        "        return None, None, None, unique_confs\n",
        "    if signal_type == 'SELL' and candidate_sl <= candidate:\n",
        "        logging.warning(\"Invalid SL for SELL.\")\n",
        "        return None, None, None, unique_confs\n",
        "\n",
        "    return candidate_conf, candidate, candidate_sl, unique_confs"
      ],
      "metadata": {
        "id": "aCRRMZZhDQXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sesssion Times"
      ],
      "metadata": {
        "id": "WTeyfXFPD57e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Utility: session filter ----------------\n",
        "def is_within_allowed_session_utc(now_utc):\n",
        "    s = now_utc.time()\n",
        "    for start_str, end_str in ALLOWED_SESSIONS_UTC:\n",
        "        start = datetime.strptime(start_str, \"%H:%M\").time()\n",
        "        end = datetime.strptime(end_str, \"%H:%M\").time()\n",
        "        if start <= s <= end:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "OdbIqt8JD7qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Signal Genaration"
      ],
      "metadata": {
        "id": "7a0fejj-D_7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Signal generation ----------------\n",
        "def generate_signals_advanced(symbol, htf_tf, ltf_tf):\n",
        "    global market_state_htf\n",
        "    htf_data = get_rates(symbol, htf_tf, LOOKBACK_CANDLES_HTF)\n",
        "    ltf_data = get_rates(symbol, ltf_tf, LOOKBACK_CANDLES_LTF)\n",
        "    if htf_data is None or ltf_data is None:\n",
        "        logging.error(\"Insufficient market data.\")\n",
        "        return None, None, None, None\n",
        "    _cache['htf_df'] = htf_data; _cache['ltf_df'] = ltf_data\n",
        "    htf_data = update_market_structure(htf_data)\n",
        "    htf_pois = find_htf_poi_with_mitigation(htf_data)\n",
        "    if not htf_pois: return None, None, None, None\n",
        "    # filter POIs aligned with HTF trend\n",
        "    aligned = []\n",
        "    for poi in htf_pois:\n",
        "        poi_type = int(poi['ob_type'])\n",
        "        if market_state_htf['trend'] == 0:\n",
        "            aligned.append(poi)\n",
        "        else:\n",
        "            if (market_state_htf['trend'] == 1 and poi_type == 1) or (market_state_htf['trend'] == -1 and poi_type == -1):\n",
        "                aligned.append(poi)\n",
        "    if not aligned:\n",
        "        logging.info(\"No POIs aligned with HTF trend.\")\n",
        "        return None, None, None, None\n",
        "    for poi in aligned:\n",
        "        signal_type = 'BUY' if int(poi['ob_type'])==1 else 'SELL'\n",
        "        conf_type, entry_price, sl_price, confs = check_ltf_confirmation_advanced(ltf_data, poi, signal_type)\n",
        "        if conf_type is not None:\n",
        "            logging.info(f\"Signal {signal_type} via {conf_type}, confluences: {confs}, entry:{entry_price}, sl:{sl_price}\")\n",
        "            return signal_type, entry_price, sl_price, conf_type\n",
        "    return None, None, None, None\n"
      ],
      "metadata": {
        "id": "6EZTdt_0D5de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backtesting"
      ],
      "metadata": {
        "id": "BdXWatmJG0Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ohlc_bt = df_1m.rename(columns={\n",
        "    'open':'Open', 'high':'High', 'low':'Low', 'close':'Close', 'volume':'Volume'\n",
        "})\n",
        "\n",
        "# Generate signals and store them in a DataFrame\n",
        "# You might need to adjust parameters for generate_signals_advanced based on your needs\n",
        "signal_list = []\n",
        "for i in range(len(ohlc_bt)):\n",
        "    # Pass a slice of the data up to the current point to the signal generation function\n",
        "    current_data_htf = df_4h.iloc[:df_4h.index.get_loc(ohlc_bt.index[i]) + 1]\n",
        "    current_data_ltf = df_1m.iloc[:i + 1]\n",
        "\n",
        "    # Ensure there is enough data for signal generation\n",
        "    if len(current_data_htf) > LOOKBACK_CANDLES_HTF and len(current_data_ltf) > LOOKBACK_CANDLES_LTF:\n",
        "        signal_type, entry_price, sl_price, conf_type = generate_signals_advanced(\"XAUUSDm\", \"4h\", \"1m\") # Use appropriate symbol and timeframes\n",
        "        if signal_type:\n",
        "            signal_list.append({\n",
        "                'DateTime': ohlc_bt.index[i],\n",
        "                'signal': signal_type,\n",
        "                'entry': entry_price,\n",
        "                'sl': sl_price,\n",
        "                'tp': np.nan, # You need to add TP calculation logic\n",
        "                'conf_type': conf_type\n",
        "            })\n",
        "\n",
        "signals_df = pd.DataFrame(signal_list).set_index('DateTime')\n",
        "\n",
        "\n",
        "class SMCStrategy(Strategy):\n",
        "    def init(self):\n",
        "        # Ensure signals_df is accessible here\n",
        "        self.signals = signals_df\n",
        "\n",
        "    def next(self):\n",
        "        dt = self.data.index[-1]\n",
        "        if dt in self.signals.index:\n",
        "            row = self.signals.loc[dt]\n",
        "            if row['signal'] == 'BUY':\n",
        "                # Check if entry price is met before placing order\n",
        "                if self.data.Close[-1] <= row['entry']:\n",
        "                     self.buy(sl=row['sl'], tp=row['tp'])\n",
        "            elif row['signal'] == 'SELL':\n",
        "                # Check if entry price is met before placing order\n",
        "                if self.data.Close[-1] >= row['entry']:\n",
        "                    self.sell(sl=row['sl'], tp=row['tp'])\n",
        "\n",
        "# =========================================================\n",
        "# 7️⃣ Run Backtest\n",
        "# =========================================================\n",
        "bt = Backtest(ohlc_bt, SMCStrategy, cash=10000, commission=0.0002, trade_on_close=True)\n",
        "stats = bt.run()\n",
        "print(stats)\n",
        "bt.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "Nk7N7hYaEHgA",
        "outputId": "c9cd66cf-9e63-4290-b1b6-a2405d4c3a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "Timestamp('2025-04-07 16:44:00')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.DatetimeEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1744044240000000000",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.DatetimeEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.DatetimeEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2025-04-07 16:44:00')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2025-04-07 16:44:00')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3549312542.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mohlc_bt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Pass a slice of the data up to the current point to the signal generation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcurrent_data_htf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_4h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf_4h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mohlc_bt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcurrent_data_ltf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_1m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_key\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatetimeTimedeltaMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_slice_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2025-04-07 16:44:00')"
          ]
        }
      ]
    }
  ]
}